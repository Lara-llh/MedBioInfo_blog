[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nThis blog will cover the content of the Applied Bioinformatics course within the MedBioInfo programme.\nUppsla University - 6th to 10th October 2025"
  },
  {
    "objectID": "posts/how-to-make-a-quarto-blog/index.html",
    "href": "posts/how-to-make-a-quarto-blog/index.html",
    "title": "How to make a Quarto blog",
    "section": "",
    "text": "In this post we are going to learn how to create a Quarto blog.\n\nPart 1: Create the blog\nThe first step is to create a Quarto blog project. In order to do so, you go to View and open the Command Palette in VScode and then type Quarto: Create Project. Select Blog project and choose the name and location in your computer.\n\n\nPart 2: Edit\nCreate a new post by generating a new folder in the posts folder of your Quarto blog. Inside this new folder create a new index.qmd file.\nThis file should contain some variations of the following information between two lines of —:\n\ntitle: “Title”\nauthor: “your name”\ndate: “YYYY-MM-DD”\ncategories: []\nimage: “image.jpg”\n\nAfter this coding information you include the content you want to have in your blog.\nThere is a lot of options for the code. Some useful resorces to explore the different alternatives that you can integrate in your code are:\n\nListings\nExecutable code\nThemes\nImages formating / Lightbox figures\n\n\n\nPart 3: Publish your blog\ngit pwd - you have to make sure you are working in the directory of your blog\n\ngit cd \"directory\" - if you need to set the working directory\n\ngit init - to initialize the blog directory as a git repository\n\ngit status - to check \n\ngit remote add origin git@github.com:user/blogname.git\n\nquarto render\n\nquarto publish gh-pages \n\nAdd the blog to GitHub using GitHub Actions\nEnter your GitHub account and create a new repository\ngit add --all - to add all the untracked files (shown with the git status command) to the repository\n\ngit commit -m \"message to track the changes\" - to commit all the changes made\n\ngit push -u origin main\n\n\n\n\n\n\nYou have to follow this three steps every time you make changes in your Quarto blog and want to push them to your GitHub repository.\n\n\n\nFor more information on how to automate the publishing of your blog you can follow this Tutorial on using GitHub Actions."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Working-with-environments/index.html",
    "href": "posts/Working-with-environments/index.html",
    "title": "Working with environments",
    "section": "",
    "text": "The different operating systems support different tools. In order to solve this issue, we can use environments, which wil help us make our bioinformatics work more reproducible. This environments specifiy the tools needed to perform the task and the managers inside the environment will make sure to install these tools with their dependencies so that you can run the analysis in your device.\nThere are different types of environments, like Conda, but in this course we are going to focus on Pixi."
  },
  {
    "objectID": "posts/Working-with-environments/index.html#install-pixi",
    "href": "posts/Working-with-environments/index.html#install-pixi",
    "title": "Working with environments",
    "section": "Install Pixi",
    "text": "Install Pixi\nIn order to install pixi just run the following command in your terminal:\ncurl -fsSL https://pixi.sh/install.sh | sh\nYou can find more infromation about pixi here"
  },
  {
    "objectID": "posts/Working-with-environments/index.html#create-an-environment",
    "href": "posts/Working-with-environments/index.html#create-an-environment",
    "title": "Working with environments",
    "section": "Create an environment",
    "text": "Create an environment\nTo create an environment for your project we will ask pixi to initialize a folder named pixi_training. We will also add conda-forge and bioconda channles with the -c flag.\npixi init pixi_training -c conda-forge -c bioconda\nIf you look in the new folder created you find a file called pixi.toml.\n\n1. pixi.toml\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"amrei_pixi_training\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nThis is how the pixi.tomlfile would like like after creating it. This file gives you information about your project:\n\nWorkspace: An overview of the environment: the channels that we added when we called pixi init -c, the name of the environment, the operating system and the version.\nTasks (not covered)\nDependencies: it will list the different tools that you install within the environment.\n\n\n\n2. pixi.lock\nAdding a tool or a dependency to the environemnt will generate another file called pixi.lock.\nThis file contains information about the channels added to you environment, where the packages were dowloaded from, liceses information and more.\n\n\n\n\n\n\nDO NOT DELETE pixi.tomlor pixi.lock files, it will break your environment!"
  },
  {
    "objectID": "posts/Working-with-environments/index.html#adding-dependencies",
    "href": "posts/Working-with-environments/index.html#adding-dependencies",
    "title": "Working with environments",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nThe comand pixi add will tell Pixi to install a specified program for you and it will be added to the dependencies list found in the pixi.toml file that we talked about in the previous section.\npwd\ncd pixi_training\npixi add quarto\nIf we now check the dependencies in the pixi.toml file we would see the new program added quarto = \"&gt;=1.7.32,&lt;2\"(the versions might be different)."
  },
  {
    "objectID": "posts/Working-with-environments/index.html#running-a-package",
    "href": "posts/Working-with-environments/index.html#running-a-package",
    "title": "Working with environments",
    "section": "Running a package",
    "text": "Running a package\nIn order to run a package we use the run command followed by the package.\npixi run quarto --help\nYou can substitute quarto with any other package followed by the pertinent commands."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to make a quatro blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nOct 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a Quarto blog\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with environments\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use and create containers\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextflow Intro\n\n\n\ninfo\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 8, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Nextflow Intro/index.html",
    "href": "posts/Nextflow Intro/index.html",
    "title": "Nextflow Intro",
    "section": "",
    "text": "Workflow managers allow you to develop an automated pipeline from your scripts that can then be run on a variety of systems.\nThe manager then coordinates the deployment of the scripts in the appropriate sequence, monitors the jobs, handles the file transfers between scripts, gathers the output, and handles re-execution of failed jobs for you. Workflow managed pipelines can run containers, which eliminates software installation and version conflicts.\nThere are two main workflow managers: snakemake and nextflow."
  },
  {
    "objectID": "posts/Nextflow Intro/index.html#netflow",
    "href": "posts/Nextflow Intro/index.html#netflow",
    "title": "Nextflow Intro",
    "section": "Netflow",
    "text": "Netflow\nIn nextflow, your scripts are turned into processes, connected by channels that contain the data - input, output etc. The order of the processes, and their interaction with each other, is specfied in the workflow scope.\n\nImportant things about nextflow: - 1. The script can be written in any language. - The modularity of the process allows to re-use existing scripts and processes easily. - 2. The functional processes are separated from the executive ones, therefore the pipelines are higly interoperable and portable. - 3. Pipelines can be very reproducible if they are integrated with version control tools (like git or bitbucket) and container technologies (apptainer or docker). - 4. They are scalable, you can strat testing with just a couple of samples and easily scale up to hundreds or thusands. Processes are run in parallele automatically when possible. - 5. It resumes executions. Autimatically checks the processes and can resume from a point of failure without having to re-compute already completed parts. - 6. It’s open source."
  },
  {
    "objectID": "posts/how-to-use-and-create-containers/index.html",
    "href": "posts/how-to-use-and-create-containers/index.html",
    "title": "How to use and create containers",
    "section": "",
    "text": "Reproducibility in bioinformatics can be a big problem. The same code can be runned by you in you computer and give some results, and when given to someone else it gives other results or it doesn’t fully work.\nIn order to resolve this issue, different tools were developed. You can use environments or containers."
  },
  {
    "objectID": "posts/how-to-use-and-create-containers/index.html#containers",
    "href": "posts/how-to-use-and-create-containers/index.html#containers",
    "title": "How to use and create containers",
    "section": "CONTAINERS",
    "text": "CONTAINERS\nThere are different programs taht can be use to build and run containers: Docker, Appptainer or Podman are the most widely used.\n\n1. How to obtain these containers\nThere are several repositories in which people publish container images, two of the most commonly used are: Dockerhub and Seqera.\n\nDockerhub\nOnce you access their webpage (no need to create an account), you can search for the software that you need. In this case we are looking for VCFtools. This software is used for VCF manipulation and querying.\n\n\n\n\n\n\nVCF manipulation and querying refers to the processes of altering (manipulating) and extracting (querying) specific information from Variant Call Format (VCF) files, which are standardized text files used in bioinformatics to store and report genomic variations in sequenced samples. - Manipulation involves functions to read, write, or modify VCF data. - Querying involves selecting and formatting subsets of the data for analysis, quality control, or visualization.\n\n\n\nAfter going to dockerhub and choosing one container, we copy the pull comand and run the following in the project server:\napptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n\nSeqera\nIn the case of Seqera, users don’t upload their containers, but they develop container images as you request them.\nTo pull an image from this repository you need to set the container setting to Singularity (Apptainer old name).\nMake sure the container is compiled before trying to pull it!!\nOnce it’s ready you can copy the text and pull it to your system with the following command:\napptainer pull vcftools_0.1.17.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\n\n\n\n2. Running Containers\nApptainer can be used to build the container from the image. Then you can either enter the container and run as if you had the exact same operating system as the person who built it, or you can run the software inside the container from outside of the container.\n\nRunning from “the outside”\napptainer exec vcftools_0.1.17.sif vcftools --version\nYou can use runor exec to use the container. Note than using runit will launch the container and first run the %runscript if one is defined and then run you command.\n\n\nRunning interactively from “the inside”\nThere is also the possibility to enter the container and work interactively within it.\napptainer shell &lt;name-of-container&gt;\nRemember that the container is a isolated system and if you want to use files from outside you will need to bind file paths using -B.\napptainer shell &lt;name-of-container&gt;\napptainer shell -B outside/path:inside/path &lt;name-of-container&gt;\nTo exit the container type exit and enter.\n\n\nRunning containers with sbatch\nApptainer containers can be run as part of a batch job if you integrate them int a SLURM job submission script.\nWe are going to add the container to our FastQCsbatch script.\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec container_image.sif fastqc -o . --noextract ../data/*fastq.gz\n\n\n\n3. Run your own container\nThis is a computationally intesnive task. The containers are build froma definition file (.defextension).\nLet´s build a container with a cow telling us the date!\nCreate a file called lolcow.def and add the following:\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat    \nThen to build the conrainer use:\napptainer build lolcow.sif lolcow.def\nYou will get information on the staus of the build and it will tell you when it´s ready.\nThen you can run your new container:\napptainer run lolcow.sif\nIf you want you can change the %runscriptfrom your lolcow.def file and change datefor fortune. Now you will get the same cow but with a tale."
  },
  {
    "objectID": "posts/Nextflow Intro/index.html#nextflow",
    "href": "posts/Nextflow Intro/index.html#nextflow",
    "title": "Nextflow Intro",
    "section": "Nextflow",
    "text": "Nextflow\nIn nextflow, your scripts are turned into processes, connected by channels that contain the data - input, output etc. The order of the processes, and their interaction with each other, is specfied in the workflow scope.\n\nImportant things about nextflow: - The script can be written in any language. - The modularity of the process allows to re-use existing scripts and processes easily. - The functional processes are separated from the executive ones, therefore the pipelines are higly interoperable and portable. - Pipelines can be very reproducible if they are integrated with version control tools (like git or bitbucket) and container technologies (apptainer or docker). - They are scalable, you can start testing with just a couple of samples and easily scale up to hundreds or thusands. Processes are run in parallele automatically when possible. - It resumes executions. Automatically checks the processes and can resume from a point of failure without having to re-compute already completed parts. - It’s open source.\n\n** Processes and Channels**\nThe Netxflow workflows are made by combining different processes. This processes can be written in different lenguages as long as they can be executed by a Linux platform. Each process is executed independently, and different processes only comunicate via a first-in, first-out asynchronous queue. These queues are called channels, and any process can defined them as input and output. The interaction between these processes and the execution of the workflow is defined by these input output declarations.\nThis is how a basic Nextflow pipeline process block would look like:\nprocess PROCESS_NAME{\n\n    input:\n      data z\n      data y\n      data x\n\n    // directives\n    container\n\n    script:\n      task1\n      task2\n      task3\n\n    output:\n      output x\n      output y\n      output z\n}\n\n\nExecution abstraction running fastQC\nIn Nextflow, the process defines what comand or script is executed, and the executor determines how that script is runned. By default, the processes are excuted on the local computer. The local executor is very useful in workflow development and testing, but when running real computational workflows other platforms are required, like a high-performance computing or cloud platform.\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz \n\n\nNextflow code example\nScript:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nTo run it:\npixi run nextflow run hello.nf\nThe output:\n N E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ✔\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ✔\nHELLO \nWORLD!\n\n\nModify and resume\nNextflow keeps track of all the processes executed and if you modify some prts of your script, only the processes that are changed will be re-executed. In this case, it will use the cached result from the processes that weren’t changed.\n\n\nWorkflow parameters\nThe workflow parameters are simply declared by prepending the prefix params to a variable name, separated by a ‘.’ (e.g. params.greetings). Their value can be specified on the comand line:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\nClean up Nextflow\nPeriodically, it makes sense to clean up your nextflow working directory. You can do that manually, but the non-descriptive nature of the file system makes that difficult.\nNextflow keeps track of your runs that have been executed from the current directory. The folowing comand will show you the executions log and runtime information of all the runs.\npixi run nextflow log\nYou can clean the porject cache and th work directories with the folowing comand:\nnextflow clean\nYou can also specify the run or runs that you want to eliminate. Here you can find a more extensive explanation of the different options."
  }
]