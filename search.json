[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nThis blog will cover the content of the Applied Bioinformatics course within the MedBioInfo programme.\nUppsla University - 6th to 10th October 2025"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Nf-core_RNAseq_pipeline/index.html",
    "href": "posts/Nf-core_RNAseq_pipeline/index.html",
    "title": "Nf-Core",
    "section": "",
    "text": "Now that we have tested nextflow and nf-core, we want to use the pipeline in our own data."
  },
  {
    "objectID": "posts/Nf-core_RNAseq_pipeline/index.html#pixi-environment",
    "href": "posts/Nf-core_RNAseq_pipeline/index.html#pixi-environment",
    "title": "Nf-Core",
    "section": "1. Pixi environment",
    "text": "1. Pixi environment\nFirst, we need to make a new directory for the analysis. What dependencies do we need for Nextflow and nf-core? Our pixi.toml file should look like this:\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"nf-core_RNAseq\"\nplatforms = [\"linux-64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nnf-core = \"&gt;=3.3.2,&lt;4\"\nnextflow = \"&gt;=25.4.8,&lt;26\""
  },
  {
    "objectID": "posts/Nf-core_RNAseq_pipeline/index.html#link-your-data-into-the-directory",
    "href": "posts/Nf-core_RNAseq_pipeline/index.html#link-your-data-into-the-directory",
    "title": "Nf-Core",
    "section": "2. Link your data into the directory",
    "text": "2. Link your data into the directory\nMake a subdirectory, data, change into it and create a symlink to the data:\nUse:\nln -s SOURCE TARGET\nSince you are in the directory data the TARGET is simply . .\nChange back to the parent directory."
  },
  {
    "objectID": "posts/Nf-core_RNAseq_pipeline/index.html#nf-core-launch",
    "href": "posts/Nf-core_RNAseq_pipeline/index.html#nf-core-launch",
    "title": "Nf-Core",
    "section": "3. nf-core launch",
    "text": "3. nf-core launch\nThe nf-core platform helps you set up the pipleines with the nf-core launcher. In order to use it, you have to go to the nf-core webpage of the pipleine you are interested in. In this case, we are going to use the nf-core/rnaseq, following this link you have to click on the button launch version 3.19.0(it could be a different one in the future). We are then redirected to a page where we can fill in all of our information about input files, as well as selecting or deselecting certain parts of the pipeline.\n\nSetting working and results directories\n\n\n\n\n\n\nWe recommend that you use absolute paths rather than relative paths for setting up your runs.\n\n\n\nDuring the first part, you need to set a working and result directory. If you are using a server that has a profile established, you can put the name of the server there. In our case, we will use the server configuration file locally.\nSet resume to True, otherwise you don’t need to change anything here. \nNext, the pipeline asks for the input CSV. Exact requirements for how that input file should look like can be found under the tab Usage on the pipeline homepage. This input CSV is unique to each analysis. The input CSV (samplesheet.csv) we need for this pipeline looks like this:\nsample,fastq_1,fastq_2,strandedness\nSRR5223504, /proj/nobackup/medbioinfo2025/lara_leal/nf-core_test_course/nf-core_RNAseq/data/SRR5223504_1.fastq.gz, /proj/nobackup/medbioinfo2025/lara_leal/nf-core_test_course/nf-core_RNAseq/data/SRR5223504_2.fastq.gz, auto\nSRR5223517, /proj/nobackup/medbioinfo2025/lara_leal/nf-core_test_course/nf-core_RNAseq/data/SRR5223517_1.fastq.gz, /proj/nobackup/medbioinfo2025/lara_leal/nf-core_test_course/nf-core_RNAseq/data/SRR5223517_2.fastq.gz, auto\n\nSet the outdir to the name and path to a directory you want the output to be saved to.\n\n\nSetting all other inputs that are required\nIn this section, you set variables that are related to your reference genome. If you are using something listed on iGenomes, you can input that name. If you are working with your own reference genome, or something not listed, you need to input the absolute path of the reference genomes you have downloaded.  \nDepending on your strategy, you might need to input a corresponding gff as well. It really depends on the kind of analysis you are hoping to perform.\nIn our case, we have human samples, so in theory we can use the iGenomes reference. However, the transcriptome and GTF files in iGenomes are out of date, so nf-core recommends downloading and using the more up-to-date version. In order to do so we are going to follow these instructions. We are going to create a reference folder in our directory that is going to contain the downloaded genome and a download.sh file with the following information:\n#!/bin/bash -l\n#SBATCH -A hpc2n2025-203\n#SBATCH -t 10:00\n#SBATCH -n 1\n \nVERSION=108\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/homo_sapiens/Homo_sapiens.GRCh38.$VERSION.gtf.gz\n\n\n\n\n\n\nOther than the options above, you don’t need to change anything to run the pipeline. However, for your project you might want to change some of the default parameters. For this exercise we will keep the rest as it is.\n\n\n\n\n\nGetting your JSON file\nOnce everything is filled in, click on Launch and you will be redirected to another page containing your JSON file that has information on your run.\nCopy the JSON file a bit lower on the screen and saving it as nf-params.json in your folder on HPC2N.\nAdd the save_reference line as recommended by nf-core (because we are using their downloaded human genome). \n\n\nConfiguration profile\nAs before, you need the HPC2N configuration file, with the correct project ID.\nAdd your e-mail under email and you will receive a message with a summary of the run."
  },
  {
    "objectID": "posts/Nf-core_RNAseq_pipeline/index.html#starting-the-run",
    "href": "posts/Nf-core_RNAseq_pipeline/index.html#starting-the-run",
    "title": "Nf-Core",
    "section": "4. Starting the run",
    "text": "4. Starting the run\nThe launcher gives us the command to run the pipeline:\nnextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json\nWe need to change this slightly, to add that we are running it via Pixi, and to add the server specific configuration file:\n\nSubmit directly via pixi\nNow you can run the pipeline with the following command (you might have to change it a bit to add pathways to files that are not in your current working directory):\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\nThere are several layers to this command:\nFirst we invoke Pixi and tell it to run the following commands.\nThen we say which program we want to run, namely Nextflow.\nThe following commands are Nextflow/ nf-core commands:\n\nwe want to run the nf-core/rnaseq pipeline, version 3.19.0\nwe want to use the parameter file called nf-params.json\nwe want to use the hpc configuration file called hpc2n.config\n\n\n\nSubmit via sbatch\nAlternatively, you can run nextflow via pixi using a batch script and slurm: copy the following text to file called name_submit_rnaseq.sh where name is your name.\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.19.0 -params-file /your_path/nf-params.json -c server.config\nAnd then submit it to slurm with:\nsbatch name_submit_rnaseq.sh\nYou can check the progress of your job with squeue -u your_username\nAnd now we wait until the run is done!\n\n\n\n\n\n\nNextflow is notoriously bad at cleaning after itself. You can check previous runs with pixi run nextflow log. And then clean up with for example pixi run nextflow clean -f -before &lt;run_name&gt;."
  },
  {
    "objectID": "posts/how-to-make-a-quarto-blog/index.html",
    "href": "posts/how-to-make-a-quarto-blog/index.html",
    "title": "How to make a Quarto blog",
    "section": "",
    "text": "In this post we are going to learn how to create a Quarto blog.\n\nPart 1: Create the blog\nThe first step is to create a Quarto blog project. In order to do so, you go to View and open the Command Palette in VScode and then type Quarto: Create Project. Select Blog project and choose the name and location in your computer.\n\n\nPart 2: Edit\nCreate a new post by generating a new folder in the posts folder of your Quarto blog. Inside this new folder create a new index.qmd file.\nThis file should contain some variations of the following information between two lines of —:\n\ntitle: “Title”\nauthor: “your name”\ndate: “YYYY-MM-DD”\ncategories: []\nimage: “image.jpg”\n\nAfter this coding information you include the content you want to have in your blog.\nThere is a lot of options for the code. Some useful resorces to explore the different alternatives that you can integrate in your code are:\n\nListings\nExecutable code\nThemes\nImages formating / Lightbox figures\n\n\n\nPart 3: Publish your blog\ngit pwd - you have to make sure you are working in the directory of your blog\n\ngit cd \"directory\" - if you need to set the working directory\n\ngit init - to initialize the blog directory as a git repository\n\ngit status - to check \n\ngit remote add origin git@github.com:user/blogname.git\n\nquarto render\n\nquarto publish gh-pages \n\nAdd the blog to GitHub using GitHub Actions\nEnter your GitHub account and create a new repository\ngit add --all - to add all the untracked files (shown with the git status command) to the repository\n\ngit commit -m \"message to track the changes\" - to commit all the changes made\n\ngit push -u origin main\n\n\n\n\n\n\nYou have to follow this three steps every time you make changes in your Quarto blog and want to push them to your GitHub repository.\n\n\n\nFor more information on how to automate the publishing of your blog you can follow this Tutorial on using GitHub Actions."
  },
  {
    "objectID": "posts/Nf-core/index.html",
    "href": "posts/Nf-core/index.html",
    "title": "Nf-Core",
    "section": "",
    "text": "Nf-core is a very active community around nextflow. All nf-core pipelines are open source and the source code is available on github. The pipelines are developed by volunteers, who can have a very varied background.\nWhile nf-core is fantastic, please be aware that their pipelines are developed and maintained (or not maintained) by the community. You should not use the pipelines as a black box, but as a tool you need to understand. The responsibility for the end results is still yours, so you need to see if your data is suited for the analysis (good enough quality?), and if the analysis is suitable for your data!"
  },
  {
    "objectID": "posts/Nf-core/index.html#why-can-these-pipelines-be-interesting-for-you",
    "href": "posts/Nf-core/index.html#why-can-these-pipelines-be-interesting-for-you",
    "title": "Nf-Core",
    "section": "Why can these pipelines be interesting for you?",
    "text": "Why can these pipelines be interesting for you?\nNf-core provides already developed pipelines for many different data sets. Likely, a pipelines exists that you can use on your data. The documentation of the pipelines follows nf-core guidelines and is extensive and informative - it is easy to understand what the pipeline does and how it works. All output is explained in detail, with links to more extensive documentation.\nOnce you have understood how to run a nf-core pipeline their consistency and standardization means you will know most about running a different one.\nUsing the nf-core launcher, will check your input, and automatically generate commands and configuration files.\nAnd this is on top of all nextflow functionality such as portability, reproducibility and the resume-at-fault option!"
  },
  {
    "objectID": "posts/Nf-core/index.html#nf-core-test-pipeline",
    "href": "posts/Nf-core/index.html#nf-core-test-pipeline",
    "title": "Nf-Core",
    "section": "nf-core: test pipeline",
    "text": "nf-core: test pipeline\nIn this section, we will create a pixi environment containing nf-core and nextflow. Once we’ve done that, we will turn our attention to nf-core to run up a pipeline with the build in test data. This is a good way to test the setup, and to familiarize yourself with the output of the pipeline.\n\nSet up your Pixi environment\nIn the course directory initialize an environment using the following command:\npixi init nextflow_test -c conda-forge -c bioconda\nChange directory into the project you created, and list the files there:\ncd name_nextflow\nls\nAdd nf-core and Nextflow:\npixi add nextflow nf-core\nAnd just check that everything worked, check the version of nextflow, get the nf-core help, and run the nextflow Hello World:\npixi run nextflow -version\npixi run nf-core --help\npixi run nextflow run hello\nWhile apptainer is sticky loaded on this server, it won’t always be the case for other servers. So, if you are running within a Linux environment (and not otherwise), you can add apptainer with the add command.\n\n\nConfiguration profile\nSince we are working on a server with a configuration profile established, but not available via nf-core, you need to download it to your working directory:\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nHere is the configuration profile on HPC2N from the above link. The most important things we need to pay attention to are the max_memory, max_cpus, and max_time settings. Your jobs won’t be able to exceed these maximum values.\nTo use the config file with nextflow you need to add our compute project under project. Use single quotes, as seen in the other entries in the file.\n// Config profile for HPC2N\nparams {\n  config_profile_description = 'Cluster profile for HPC2N'\n  config_profile_contact = '----'\n  config_profile_url = 'https://www.hpc2n.umu.se/'\n  project = null\n  clusterOptions = null\n  max_memory = 128.GB\n  max_cpus = 28\n  max_time = 168.h\n  email = '-----'\n}\n\nsingularity {\n  enabled = true\n}\n\nprocess {\n  executor = 'slurm'\n  clusterOptions = { \"-A $params.project ${params.clusterOptions ?: ''}\" }\n}\n\n\nRun the pipeline with a test profile\nAs an example we willl run a Sarek pipeline, which is a variant calling pipeline:\npixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c hpc2n.config\nLet’s have a look at the components of the above command:\n\npixi run: we are using our pixi environment to run the following commands\nnextflow run: run the following with nextflow\nnf-core/sarek: name/ location of the pipeline\n-profile test: use the test profile for this run - this uses the build in test data etc. On servers with a nf-core configuration file you would list the name of the profile here.\n--outdir sarek_test: name and location of the directory for the pipeline output\n-c hpc2n.config: name and location of the configuration file for the server"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to make a quatro blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nOct 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a Quarto blog\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with environments\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use and create containers\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextflow Intro\n\n\n\ninfo\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNf-Core\n\n\n\ninfo\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNf-Core\n\n\n\ninfo\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextflow RNAseq pipeline\n\n\n\ninfo\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 9, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html",
    "title": "Nextflow RNAseq pipeline",
    "section": "",
    "text": "In a real-world biomedical example, we will implement a proof of concept workflow that:\nIn order to do so, 7 scripts will be used, an each of them builds upon the other. The scripts will use the following third-party tools:"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#set-the-executor",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#set-the-executor",
    "title": "Nextflow RNAseq pipeline",
    "section": "Set the executor",
    "text": "Set the executor\nSo far we have used the local executor, but the pipeline we are going to work with now is more complicated and requires more computing power. We will set slurm as the executor, in order to do so we will change it in a file called nextflow.config. It should loook like the following:\n process{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = 'your_project_account'\n}\nWith this script we:\n\nSet the executor for every process to slurm.\nSet resume to true, so it is automatically used for all our runs (which means we do not have to specify this in our nextflow run command anymore).\nEnable singularity and set some singularity run options.\nSpecify the account name for the slurm execution.\n\nThis nextflow.config file is implicitly called when we execute nextflow from the folder it is in. We don’t need to explicitily name it in the run command."
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#define-the-workflow-parameters",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#define-the-workflow-parameters",
    "title": "Nextflow RNAseq pipeline",
    "section": "Define the workflow parameters",
    "text": "Define the workflow parameters\nParameter are inputs and options that can be changed when the workflow is executed.\nAn example found in script1.nf of the training material is:\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nprintln \"reads: $params.reads\"\nAnd it is run with the following command:\npixi run nextflow run script1.nf\nNow we are going to modify the script adding a fourth parameter called outdir and we will set it to the default path that will be used as the workflow output directory.\n#!/usr/bin/env nextflow\n\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nparams.outdir = \"$projectDir/results\"\n\nprintln \"reads: $params.outdir\""
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#create-a-transcriptome-index-file",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#create-a-transcriptome-index-file",
    "title": "Nextflow RNAseq pipeline",
    "section": "Create a transcriptome index file",
    "text": "Create a transcriptome index file\nIn Nextflow you can execute any command or script by using a process definition. A process is defined by providing 3 main declarations: the input, the output and the command script.\nLet’s add a transcriptome index processing step:\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nparams.outdir = \"$projectDir/results\"\n\nprintln \"reads: $params.outdir\"\n\nprocess INDEX {\n    input:\n    path transcriptome\n    \n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\nAdditionally, we have to add a workflow scope containing an input channel definiton and the index process:\nworkflow {\n    index_ch = INDEX(file(params.transcriptome_file, checkIfExists: true))\n}\nIn this case the params.transcriptome_file parameter is used as the input for the index process. The index process (using salmon) creates salmon_index, an indexed transcriptome that is passed as an output to the index_ch channel.\nTo run it we will use:\npixi run nextflow run script1.nf \nThe workflow will not work because first we have to add Salmon to our environment.\npixi add salmon\nAdding a container to the process and running the tool via the container is more reproducible than adding it to your environment. You can do so the following way:\ncontainer 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\nAdditonally, you can also specify a process specific output directory in the process block.\npublishDir \"$params.outdir/salmon\"\nNextflow is big on execution abstraction. Therefore, we will specify the allocated time and cpus for this specific process in the nextflow.config file.\nwithName:'INDEX'{\n    time = 15.m\n    cpus = 2\n}\nThe final version of script1.nf should look like this:\n#!/usr/bin/env nextflow\n\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nparams.outdir = \"$projectDir/results\"\n\nprintln \"reads: $params.outdir\"\n\nprocess INDEX {\n    input:\n    path transcriptome\n    \n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon\"\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n\nworkflow {\n    index_ch = INDEX(file(params.transcriptome_file, checkIfExists: true))\n}\nThe nextflow.config file should look like this:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n        \n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = ''\n}\nRun it using the following command:\npixi run nextflow run script1.nf \nAnother option for this analysis is the following script3.nf\n#!/usr/bin/env nextflow\n\n/*\n * pipeline input parameters\n */\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n\nlog.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent()\n\nread_pairs_ch = Channel.fromFilePairs(params.reads)\nread_pairs_ch.view()\nThe read_pairs_ch.view() command allows us to see how the read_pair_ch channel emits tuples composed of two elements, where the first is the read pair prefix and the second is a list representing actual files. It will print something similar to this:\n[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n*Tuple: a data structure consisting of multiple parts."
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#expression-quantification",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#expression-quantification",
    "title": "Nextflow RNAseq pipeline",
    "section": "Expression quantification",
    "text": "Expression quantification\nWe will add a gene expression quantification process to the script and a call to it within the workflow scope. Quantification requires the index transcriptome and RNA-seq read pair fastq files.\n#!/usr/bin/env nextflow\n\n/*\n * pipeline input parameters\n */\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n\nlog.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent()\n\n/*\n * define the `INDEX` process that creates a binary index\n * given the transcriptome file\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_INDEX\"\n\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_quantification\"\n\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    multiqc_ch = MULTIQC(quant_ch.mix(fastqc_ch).collect())\n}\nIn the workflow scope, note how the index_ch channel is assigned as output in the INDEX process.\nNext, note that the first input channel for the quantification process is the previously declared index_ch, which contains the path to the salmon_index.\nAlso, note that the second input channel for the quantification process, is the read_pair_ch we just created. This being a tuple composed of two elements (a value: “sample_id” and a list of paths to the fastq reads: “reads”) in order to match the structure of the items emitted by the fromFilePairs channel factory.\nThe script can be run using:\npixi run nextflow run script4.nf \nThe same script can be executed with more read files, as shown below:\npixi run nextflow run script4.nf --reads 'data/ggal/*_{1,2}.fq'\nThe quantification process will be executed multiple times. Nextflow parallelizes the execution of your workflow by providing multiple sets of input data to your script.\nThe process specific runtime environment definition for quantification can be added to the nextflow.config file, leaving the file as follow:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n        \n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\n\n    withName:'QUANTIFICATION'{\n        time = 10.m\n        cpus = 2\n    }\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = ''\n}"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#quality-control",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#quality-control",
    "title": "Nextflow RNAseq pipeline",
    "section": "Quality control",
    "text": "Quality control\nNow we want to add another process using the FastQC to check the samples. The input is the same as the read pairs used in the quantification step.\nIn the script4.nf file we will have to add the following after the quantification process:\nprocess FASTQC {\n    input:\n    tuple val(sample_id), path(reads)\n\n    container 'oras://community.wave.seqera.io/library/fastqc:0.12.1--104d26ddd9519960'\n    publishDir \"$params.outdir/fastqc\"\n\n    output:\n    path \"fastqc_$sample_id\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}\n    fastqc --noextract -o fastqc_${sample_id} ${reads[0]} ${reads[1]} \n    \"\"\"\n}    \nAnd the workflow needs to be updated aswell.\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#multiqc",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#multiqc",
    "title": "Nextflow RNAseq pipeline",
    "section": "MultiQC",
    "text": "MultiQC\nAs a final step we will use MultiQC to generate a final report that will collect the outputs from the quantification and the FastQC processes.\nIn the script4.nf we will add a new process:\nprocess MULTIQC {\n    input:\n    path '*'\n\n    container 'community.wave.seqera.io/library/multiqc:1.31--1efbafd542a23882'\n    publishDir \"$params.outdir/multiqc\"\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}    \nAnd we will also have to update the workflow:\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    multiqc_ch = MULTIQC(quant_ch.mix(fastqc_ch).collect())\n}"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#scripts-summary",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#scripts-summary",
    "title": "Nextflow RNAseq pipeline",
    "section": "SCRIPTS SUMMARY",
    "text": "SCRIPTS SUMMARY\nOur final script4.nf should look like this:\n#!/usr/bin/env nextflow\n\n/*\n * pipeline input parameters\n */\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n\nlog.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent()\n\n/*\n * define the `INDEX` process that creates a binary index\n * given the transcriptome file\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_INDEX\"\n\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_quantification\"\n\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    input:\n    tuple val(sample_id), path(reads)\n\n    container 'oras://community.wave.seqera.io/library/fastqc:0.12.1--104d26ddd9519960'\n    publishDir \"$params.outdir/fastqc\"\n\n    output:\n    path \"fastqc_$sample_id\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}\n    fastqc --noextract -o fastqc_${sample_id} ${reads[0]} ${reads[1]} \n    \"\"\"\n}    \n\nprocess MULTIQC {\n    input:\n    path '*'\n\n    container 'community.wave.seqera.io/library/multiqc:1.31--1efbafd542a23882'\n    publishDir \"$params.outdir/multiqc\"\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}    \n\n\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    multiqc_ch = MULTIQC(quant_ch.mix(fastqc_ch).collect())\n}\nAnd the nextflow.config file:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n        \n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\n\n    withName:'QUANTIFICATION'{\n        time = 10.m\n        cpus = 2\n    }\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = ''\n}"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/how-to-use-and-create-containers/index.html",
    "href": "posts/how-to-use-and-create-containers/index.html",
    "title": "How to use and create containers",
    "section": "",
    "text": "Reproducibility in bioinformatics can be a big problem. The same code can be runned by you in you computer and give some results, and when given to someone else it gives other results or it doesn’t fully work.\nIn order to resolve this issue, different tools were developed. You can use environments or containers."
  },
  {
    "objectID": "posts/how-to-use-and-create-containers/index.html#containers",
    "href": "posts/how-to-use-and-create-containers/index.html#containers",
    "title": "How to use and create containers",
    "section": "CONTAINERS",
    "text": "CONTAINERS\nThere are different programs taht can be use to build and run containers: Docker, Appptainer or Podman are the most widely used.\n\n1. How to obtain these containers\nThere are several repositories in which people publish container images, two of the most commonly used are: Dockerhub and Seqera.\n\nDockerhub\nOnce you access their webpage (no need to create an account), you can search for the software that you need. In this case we are looking for VCFtools. This software is used for VCF manipulation and querying.\n\n\n\n\n\n\nVCF manipulation and querying refers to the processes of altering (manipulating) and extracting (querying) specific information from Variant Call Format (VCF) files, which are standardized text files used in bioinformatics to store and report genomic variations in sequenced samples. - Manipulation involves functions to read, write, or modify VCF data. - Querying involves selecting and formatting subsets of the data for analysis, quality control, or visualization.\n\n\n\nAfter going to dockerhub and choosing one container, we copy the pull comand and run the following in the project server:\napptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n\nSeqera\nIn the case of Seqera, users don’t upload their containers, but they develop container images as you request them.\nTo pull an image from this repository you need to set the container setting to Singularity (Apptainer old name).\nMake sure the container is compiled before trying to pull it!!\nOnce it’s ready you can copy the text and pull it to your system with the following command:\napptainer pull vcftools_0.1.17.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\n\n\n\n2. Running Containers\nApptainer can be used to build the container from the image. Then you can either enter the container and run as if you had the exact same operating system as the person who built it, or you can run the software inside the container from outside of the container.\n\nRunning from “the outside”\napptainer exec vcftools_0.1.17.sif vcftools --version\nYou can use runor exec to use the container. Note than using runit will launch the container and first run the %runscript if one is defined and then run you command.\n\n\nRunning interactively from “the inside”\nThere is also the possibility to enter the container and work interactively within it.\napptainer shell &lt;name-of-container&gt;\nRemember that the container is a isolated system and if you want to use files from outside you will need to bind file paths using -B.\napptainer shell &lt;name-of-container&gt;\napptainer shell -B outside/path:inside/path &lt;name-of-container&gt;\nTo exit the container type exit and enter.\n\n\nRunning containers with sbatch\nApptainer containers can be run as part of a batch job if you integrate them int a SLURM job submission script.\nWe are going to add the container to our FastQCsbatch script.\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec container_image.sif fastqc -o . --noextract ../data/*fastq.gz\n\n\n\n3. Run your own container\nThis is a computationally intesnive task. The containers are build froma definition file (.defextension).\nLet´s build a container with a cow telling us the date!\nCreate a file called lolcow.def and add the following:\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat    \nThen to build the conrainer use:\napptainer build lolcow.sif lolcow.def\nYou will get information on the staus of the build and it will tell you when it´s ready.\nThen you can run your new container:\napptainer run lolcow.sif\nIf you want you can change the %runscriptfrom your lolcow.def file and change datefor fortune. Now you will get the same cow but with a tale."
  },
  {
    "objectID": "posts/Nextflow_Intro/index.html",
    "href": "posts/Nextflow_Intro/index.html",
    "title": "Nextflow Intro",
    "section": "",
    "text": "Workflow managers allow you to develop an automated pipeline from your scripts that can then be run on a variety of systems.\nThe manager then coordinates the deployment of the scripts in the appropriate sequence, monitors the jobs, handles the file transfers between scripts, gathers the output, and handles re-execution of failed jobs for you. Workflow managed pipelines can run containers, which eliminates software installation and version conflicts.\nThere are two main workflow managers: snakemake and nextflow."
  },
  {
    "objectID": "posts/Nextflow_Intro/index.html#nextflow",
    "href": "posts/Nextflow_Intro/index.html#nextflow",
    "title": "Nextflow Intro",
    "section": "Nextflow",
    "text": "Nextflow\nIn nextflow, your scripts are turned into processes, connected by channels that contain the data - input, output etc. The order of the processes, and their interaction with each other, is specfied in the workflow scope.\n\nImportant things about nextflow:\n\nThe script can be written in any language.\nThe modularity of the process allows to re-use existing scripts and processes easily.\nThe functional processes are separated from the executive ones, therefore the pipelines are higly interoperable and portable.\nPipelines can be very reproducible if they are integrated with version control tools (like git or bitbucket) and container technologies (apptainer or docker).\nThey are scalable, you can start testing with just a couple of samples and easily scale up to hundreds or thusands. Processes are run in parallele automatically when possible.\nIt resumes executions. Automatically checks the processes and can resume from a point of failure without having to re-compute already completed parts.\nIt’s open source.\n\n\nProcesses and Channels\nThe Netxflow workflows are made by combining different processes. This processes can be written in different lenguages as long as they can be executed by a Linux platform. Each process is executed independently, and different processes only comunicate via a first-in, first-out asynchronous queue. These queues are called channels, and any process can defined them as input and output. The interaction between these processes and the execution of the workflow is defined by these input output declarations.\nThis is how a basic Nextflow pipeline process block would look like:\nprocess PROCESS_NAME{\n\n    input:\n      data z\n      data y\n      data x\n\n    // directives\n    container\n\n    script:\n      task1\n      task2\n      task3\n\n    output:\n      output x\n      output y\n      output z\n}\n\n\nExecution abstraction running fastQC\nIn Nextflow, the process defines what comand or script is executed, and the executor determines how that script is runned. By default, the processes are excuted on the local computer. The local executor is very useful in workflow development and testing, but when running real computational workflows other platforms are required, like a high-performance computing or cloud platform.\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz \n\n\nNextflow code example\nScript:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nTo run it:\npixi run nextflow run hello.nf\nThe output:\n N E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ✔\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ✔\nHELLO \nWORLD!\n\n\nModify and resume\nNextflow keeps track of all the processes executed and if you modify some prts of your script, only the processes that are changed will be re-executed. In this case, it will use the cached result from the processes that weren’t changed.\n\n\nWorkflow parameters\nThe workflow parameters are simply declared by prepending the prefix params to a variable name, separated by a ‘.’ (e.g. params.greetings). Their value can be specified on the comand line:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\nClean up Nextflow\nPeriodically, it makes sense to clean up your nextflow working directory. You can do that manually, but the non-descriptive nature of the file system makes that difficult.\nNextflow keeps track of your runs that have been executed from the current directory. The folowing comand will show you the executions log and runtime information of all the runs.\npixi run nextflow log\nYou can clean the porject cache and th work directories with the folowing comand:\nnextflow clean\nYou can also specify the run or runs that you want to eliminate. Here you can find a more extensive explanation of the different options."
  },
  {
    "objectID": "posts/Working-with-environments/index.html",
    "href": "posts/Working-with-environments/index.html",
    "title": "Working with environments",
    "section": "",
    "text": "The different operating systems support different tools. In order to solve this issue, we can use environments, which wil help us make our bioinformatics work more reproducible. This environments specifiy the tools needed to perform the task and the managers inside the environment will make sure to install these tools with their dependencies so that you can run the analysis in your device.\nThere are different types of environments, like Conda, but in this course we are going to focus on Pixi."
  },
  {
    "objectID": "posts/Working-with-environments/index.html#install-pixi",
    "href": "posts/Working-with-environments/index.html#install-pixi",
    "title": "Working with environments",
    "section": "Install Pixi",
    "text": "Install Pixi\nIn order to install pixi just run the following command in your terminal:\ncurl -fsSL https://pixi.sh/install.sh | sh\nYou can find more infromation about pixi here"
  },
  {
    "objectID": "posts/Working-with-environments/index.html#create-an-environment",
    "href": "posts/Working-with-environments/index.html#create-an-environment",
    "title": "Working with environments",
    "section": "Create an environment",
    "text": "Create an environment\nTo create an environment for your project we will ask pixi to initialize a folder named pixi_training. We will also add conda-forge and bioconda channles with the -c flag.\npixi init pixi_training -c conda-forge -c bioconda\nIf you look in the new folder created you find a file called pixi.toml.\n\n1. pixi.toml\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"amrei_pixi_training\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nThis is how the pixi.tomlfile would like like after creating it. This file gives you information about your project:\n\nWorkspace: An overview of the environment: the channels that we added when we called pixi init -c, the name of the environment, the operating system and the version.\nTasks (not covered)\nDependencies: it will list the different tools that you install within the environment.\n\n\n\n2. pixi.lock\nAdding a tool or a dependency to the environemnt will generate another file called pixi.lock.\nThis file contains information about the channels added to you environment, where the packages were dowloaded from, liceses information and more.\n\n\n\n\n\n\nDO NOT DELETE pixi.tomlor pixi.lock files, it will break your environment!"
  },
  {
    "objectID": "posts/Working-with-environments/index.html#adding-dependencies",
    "href": "posts/Working-with-environments/index.html#adding-dependencies",
    "title": "Working with environments",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nThe comand pixi add will tell Pixi to install a specified program for you and it will be added to the dependencies list found in the pixi.toml file that we talked about in the previous section.\npwd\ncd pixi_training\npixi add quarto\nIf we now check the dependencies in the pixi.toml file we would see the new program added quarto = \"&gt;=1.7.32,&lt;2\"(the versions might be different)."
  },
  {
    "objectID": "posts/Working-with-environments/index.html#running-a-package",
    "href": "posts/Working-with-environments/index.html#running-a-package",
    "title": "Working with environments",
    "section": "Running a package",
    "text": "Running a package\nIn order to run a package we use the run command followed by the package.\npixi run quarto --help\nYou can substitute quarto with any other package followed by the pertinent commands."
  }
]