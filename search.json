[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nThis blog will cover the content of the Applied Bioinformatics course within the MedBioInfo programme.\nUppsla University - 6th to 10th October 2025"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/how-to-use-and-create-containers/index.html",
    "href": "posts/how-to-use-and-create-containers/index.html",
    "title": "How to use and create containers",
    "section": "",
    "text": "Reproducibility in bioinformatics can be a big problem. The same code can be runned by you in you computer and give some results, and when given to someone else it gives other results or it doesn’t fully work.\nIn order to resolve this issue, different tools were developed. You can use environments or containers."
  },
  {
    "objectID": "posts/how-to-use-and-create-containers/index.html#containers",
    "href": "posts/how-to-use-and-create-containers/index.html#containers",
    "title": "How to use and create containers",
    "section": "CONTAINERS",
    "text": "CONTAINERS\nThere are different programs taht can be use to build and run containers: Docker, Appptainer or Podman are the most widely used.\n\n1. How to obtain these containers\nThere are several repositories in which people publish container images, two of the most commonly used are: Dockerhub and Seqera.\n\nDockerhub\nOnce you access their webpage (no need to create an account), you can search for the software that you need. In this case we are looking for VCFtools. This software is used for VCF manipulation and querying.\n\n\n\n\n\n\nVCF manipulation and querying refers to the processes of altering (manipulating) and extracting (querying) specific information from Variant Call Format (VCF) files, which are standardized text files used in bioinformatics to store and report genomic variations in sequenced samples. - Manipulation involves functions to read, write, or modify VCF data. - Querying involves selecting and formatting subsets of the data for analysis, quality control, or visualization.\n\n\n\nAfter going to dockerhub and choosing one container, we copy the pull comand and run the following in the project server:\napptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n\nSeqera\nIn the case of Seqera, users don’t upload their containers, but they develop container images as you request them.\nTo pull an image from this repository you need to set the container setting to Singularity (Apptainer old name).\nMake sure the container is compiled before trying to pull it!!\nOnce it’s ready you can copy the text and pull it to your system with the following command:\napptainer pull vcftools_0.1.17.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\n\n\n\n2. Running Containers\nApptainer can be used to build the container from the image. Then you can either enter the container and run as if you had the exact same operating system as the person who built it, or you can run the software inside the container from outside of the container.\n\nRunning from “the outside”\napptainer exec vcftools_0.1.17.sif vcftools --version\nYou can use runor exec to use the container. Note than using runit will launch the container and first run the %runscript if one is defined and then run you command.\n\n\nRunning interactively from “the inside”\nThere is also the possibility to enter the container and work interactively within it.\napptainer shell &lt;name-of-container&gt;\nRemember that the container is a isolated system and if you want to use files from outside you will need to bind file paths using -B.\napptainer shell &lt;name-of-container&gt;\napptainer shell -B outside/path:inside/path &lt;name-of-container&gt;\nTo exit the container type exit and enter.\n\n\nRunning containers with sbatch\nApptainer containers can be run as part of a batch job if you integrate them int a SLURM job submission script.\nWe are going to add the container to our FastQCsbatch script.\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec container_image.sif fastqc -o . --noextract ../data/*fastq.gz\n\n\n\n3. Run your own container\nThis is a computationally intesnive task. The containers are build froma definition file (.defextension).\nLet´s build a container with a cow telling us the date!\nCreate a file called lolcow.def and add the following:\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat    \nThen to build the conrainer use:\napptainer build lolcow.sif lolcow.def\nYou will get information on the staus of the build and it will tell you when it´s ready.\nThen you can run your new container:\napptainer run lolcow.sif\nIf you want you can change the %runscriptfrom your lolcow.def file and change datefor fortune. Now you will get the same cow but with a tale."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to make a quatro blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nOct 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a Quarto blog\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with environments\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use and create containers\n\n\n\ncode\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextflow Intro\n\n\n\ninfo\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNextflow RNAseq pipeline\n\n\n\ninfo\n\n\n\n\n\n\n\nLara Leal\n\n\nOct 9, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html",
    "title": "Nextflow RNAseq pipeline",
    "section": "",
    "text": "In a real-world biomedical example, we will implement a proof of concept workflow that:\nIn order to do so, 7 scripts will be used, an each of them builds upon the other. The scripts will use the following third-party tools:"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#set-the-executor",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#set-the-executor",
    "title": "Nextflow RNAseq pipeline",
    "section": "Set the executor",
    "text": "Set the executor\nSo far we have used the local executor, but the pipeline we are going to work with now is more complicated and requires more computing power. We will set slurm as the executor, in order to do so we will change it in a file called nextflow.config. It should loook like the following:\n process{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = 'your_project_account'\n}\nWith this script we:\n- Set the executor for every process to slurm.\n- Set resume to true, so it is automatically used for all our runs (which means we do not have to specify this in our nextflow run command anymore).\n- Enable singularity and set some singularity run options.\n- Specify the account name for the slurm execution.\nThis nextflow.config file is implicitly called when we execute nextflow from the folder it is in. We don’t need to explicitily name it in the run command."
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#define-the-workflow-parameters",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#define-the-workflow-parameters",
    "title": "Nextflow RNAseq pipeline",
    "section": "Define the workflow parameters",
    "text": "Define the workflow parameters\nParameter are inputs and options that can be changed when the workflow is executed.\nAn example found in script1.nf of the training material is:\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nprintln \"reads: $params.reads\"\nAnd it is run with the following command:\npixi run nextflow run script1.nf\nNow we are going to modify the script adding a fourth parameter called outdir and we will set it to the default path that will be used as the workflow output directory.\n#!/usr/bin/env nextflow\n\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nparams.outdir = \"$projectDir/results\"\n\nprintln \"reads: $params.outdir\""
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#create-a-transcriptome-index-file",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#create-a-transcriptome-index-file",
    "title": "Nextflow RNAseq pipeline",
    "section": "Create a transcriptome index file",
    "text": "Create a transcriptome index file\nIn Nextflow you can execute any command or script by using a process definition. A process is defined by providing 3 main declarations: the input, the output and the command script.\nLet’s add a transcriptome index processing step:\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nparams.outdir = \"$projectDir/results\"\n\nprintln \"reads: $params.outdir\"\n\nprocess INDEX {\n    input:\n    path transcriptome\n    \n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\nAdditionally, we have to add a workflow scope containing an input channel definiton and the index process:\nworkflow {\n    index_ch = INDEX(file(params.transcriptome_file, checkIfExists: true))\n}\nIn this case the params.transcriptome_file parameter is used as the input for the index process. The index process (using salmon) creates salmon_index, an indexed transcriptome that is passed as an output to the index_ch channel.\nTo run it we will use:\npixi run nextflow run script1.nf \nThe workflow will not work because first we have to add Salmon to our environment.\npixi add salmon\nAdding a container to the process and running the tool via the container is more reproducible than adding it to your environment. You can do so the following way:\ncontainer 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\nAdditonally, you can also specify a process specific output directory in the process block.\npublishDir \"$params.outdir/salmon\"\nNextflow is big on execution abstraction. Therefore, we will specify the allocated time and cpus for this specific process in the nextflow.config file.\nwithName:'INDEX'{\n    time = 15.m\n    cpus = 2\n}\nThe final version of script1.nf should look like this:\n#!/usr/bin/env nextflow\n\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nparams.outdir = \"$projectDir/results\"\n\nprintln \"reads: $params.outdir\"\n\nprocess INDEX {\n    input:\n    path transcriptome\n    \n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon\"\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n\nworkflow {\n    index_ch = INDEX(file(params.transcriptome_file, checkIfExists: true))\n}\nThe nextflow.config file should look like this:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n        \n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = ''\n}\nRun it using the following command:\npixi run nextflow run script1.nf \nAnother option for this analysis is the following script3.nf\n#!/usr/bin/env nextflow\n\n/*\n * pipeline input parameters\n */\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n\nlog.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent()\n\nread_pairs_ch = Channel.fromFilePairs(params.reads)\nread_pairs_ch.view()\nThe read_pairs_ch.view() command allows us to see how the read_pair_ch channel emits tuples composed of two elements, where the first is the read pair prefix and the second is a list representing actual files. It will print something similar to this:\n[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n*Tuple: a data structure consisting of multiple parts."
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#expression-quantification",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#expression-quantification",
    "title": "Nextflow RNAseq pipeline",
    "section": "Expression quantification",
    "text": "Expression quantification\nWe will add a gene expression quantification process to the script and a call to it within the workflow scope. Quantification requires the index transcriptome and RNA-seq read pair fastq files.\n#!/usr/bin/env nextflow\n\n/*\n * pipeline input parameters\n */\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n\nlog.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent()\n\n/*\n * define the `INDEX` process that creates a binary index\n * given the transcriptome file\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_INDEX\"\n\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_quantification\"\n\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    multiqc_ch = MULTIQC(quant_ch.mix(fastqc_ch).collect())\n}\nIn the workflow scope, note how the index_ch channel is assigned as output in the INDEX process.\nNext, note that the first input channel for the quantification process is the previously declared index_ch, which contains the path to the salmon_index.\nAlso, note that the second input channel for the quantification process, is the read_pair_ch we just created. This being a tuple composed of two elements (a value: “sample_id” and a list of paths to the fastq reads: “reads”) in order to match the structure of the items emitted by the fromFilePairs channel factory.\nThe script can be run using:\npixi run nextflow run script4.nf \nThe same script can be executed with more read files, as shown below:\npixi run nextflow run script4.nf --reads 'data/ggal/*_{1,2}.fq'\nThe quantification process will be executed multiple times. Nextflow parallelizes the execution of your workflow by providing multiple sets of input data to your script.\nThe process specific runtime environment definition for quantification can be added to the nextflow.config file, leaving the file as follow:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n        \n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\n\n    withName:'QUANTIFICATION'{\n        time = 10.m\n        cpus = 2\n    }\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = ''\n}"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#quality-control",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#quality-control",
    "title": "Nextflow RNAseq pipeline",
    "section": "Quality control",
    "text": "Quality control\nNow we want to add another process using the FastQC to check the samples. The input is the same as the read pairs used in the quantification step.\nIn the script4.nf file we will have to add the following after the quantification process:\nprocess FASTQC {\n    input:\n    tuple val(sample_id), path(reads)\n\n    container 'oras://community.wave.seqera.io/library/fastqc:0.12.1--104d26ddd9519960'\n    publishDir \"$params.outdir/fastqc\"\n\n    output:\n    path \"fastqc_$sample_id\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}\n    fastqc --noextract -o fastqc_${sample_id} ${reads[0]} ${reads[1]} \n    \"\"\"\n}    \nAnd the workflow needs to be updated aswell.\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#multiqc",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#multiqc",
    "title": "Nextflow RNAseq pipeline",
    "section": "MultiQC",
    "text": "MultiQC\nAs a final step we will use MultiQC to generate a final report that will collect the outputs from the quantification and the FastQC processes.\nIn the script4.nf we will add a new process:\nprocess MULTIQC {\n    input:\n    path '*'\n\n    container 'community.wave.seqera.io/library/multiqc:1.31--1efbafd542a23882'\n    publishDir \"$params.outdir/multiqc\"\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}    \nAnd we will also have to update the workflow:\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    multiqc_ch = MULTIQC(quant_ch.mix(fastqc_ch).collect())\n}"
  },
  {
    "objectID": "posts/Nextflow_RNAseq_pipeline/index.html#scripts-summary",
    "href": "posts/Nextflow_RNAseq_pipeline/index.html#scripts-summary",
    "title": "Nextflow RNAseq pipeline",
    "section": "SCRIPTS SUMMARY",
    "text": "SCRIPTS SUMMARY\nOur final script4.nf should look like this:\n#!/usr/bin/env nextflow\n\n/*\n * pipeline input parameters\n */\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n\nlog.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent()\n\n/*\n * define the `INDEX` process that creates a binary index\n * given the transcriptome file\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_INDEX\"\n\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_quantification\"\n\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    input:\n    tuple val(sample_id), path(reads)\n\n    container 'oras://community.wave.seqera.io/library/fastqc:0.12.1--104d26ddd9519960'\n    publishDir \"$params.outdir/fastqc\"\n\n    output:\n    path \"fastqc_$sample_id\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}\n    fastqc --noextract -o fastqc_${sample_id} ${reads[0]} ${reads[1]} \n    \"\"\"\n}    \n\nprocess MULTIQC {\n    input:\n    path '*'\n\n    container 'community.wave.seqera.io/library/multiqc:1.31--1efbafd542a23882'\n    publishDir \"$params.outdir/multiqc\"\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}    \n\n\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    multiqc_ch = MULTIQC(quant_ch.mix(fastqc_ch).collect())\n}\nAnd the nextflow.config file:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n        \n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\n\n    withName:'QUANTIFICATION'{\n        time = 10.m\n        cpus = 2\n    }\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = ''\n}"
  },
  {
    "objectID": "posts/how-to-make-a-quarto-blog/index.html",
    "href": "posts/how-to-make-a-quarto-blog/index.html",
    "title": "How to make a Quarto blog",
    "section": "",
    "text": "In this post we are going to learn how to create a Quarto blog.\n\nPart 1: Create the blog\nThe first step is to create a Quarto blog project. In order to do so, you go to View and open the Command Palette in VScode and then type Quarto: Create Project. Select Blog project and choose the name and location in your computer.\n\n\nPart 2: Edit\nCreate a new post by generating a new folder in the posts folder of your Quarto blog. Inside this new folder create a new index.qmd file.\nThis file should contain some variations of the following information between two lines of —:\n\ntitle: “Title”\nauthor: “your name”\ndate: “YYYY-MM-DD”\ncategories: []\nimage: “image.jpg”\n\nAfter this coding information you include the content you want to have in your blog.\nThere is a lot of options for the code. Some useful resorces to explore the different alternatives that you can integrate in your code are:\n\nListings\nExecutable code\nThemes\nImages formating / Lightbox figures\n\n\n\nPart 3: Publish your blog\ngit pwd - you have to make sure you are working in the directory of your blog\n\ngit cd \"directory\" - if you need to set the working directory\n\ngit init - to initialize the blog directory as a git repository\n\ngit status - to check \n\ngit remote add origin git@github.com:user/blogname.git\n\nquarto render\n\nquarto publish gh-pages \n\nAdd the blog to GitHub using GitHub Actions\nEnter your GitHub account and create a new repository\ngit add --all - to add all the untracked files (shown with the git status command) to the repository\n\ngit commit -m \"message to track the changes\" - to commit all the changes made\n\ngit push -u origin main\n\n\n\n\n\n\nYou have to follow this three steps every time you make changes in your Quarto blog and want to push them to your GitHub repository.\n\n\n\nFor more information on how to automate the publishing of your blog you can follow this Tutorial on using GitHub Actions."
  },
  {
    "objectID": "posts/Nextflow_Intro/index.html",
    "href": "posts/Nextflow_Intro/index.html",
    "title": "Nextflow Intro",
    "section": "",
    "text": "Workflow managers allow you to develop an automated pipeline from your scripts that can then be run on a variety of systems.\nThe manager then coordinates the deployment of the scripts in the appropriate sequence, monitors the jobs, handles the file transfers between scripts, gathers the output, and handles re-execution of failed jobs for you. Workflow managed pipelines can run containers, which eliminates software installation and version conflicts.\nThere are two main workflow managers: snakemake and nextflow."
  },
  {
    "objectID": "posts/Nextflow_Intro/index.html#nextflow",
    "href": "posts/Nextflow_Intro/index.html#nextflow",
    "title": "Nextflow Intro",
    "section": "Nextflow",
    "text": "Nextflow\nIn nextflow, your scripts are turned into processes, connected by channels that contain the data - input, output etc. The order of the processes, and their interaction with each other, is specfied in the workflow scope.\n\nImportant things about nextflow: - The script can be written in any language. - The modularity of the process allows to re-use existing scripts and processes easily. - The functional processes are separated from the executive ones, therefore the pipelines are higly interoperable and portable. - Pipelines can be very reproducible if they are integrated with version control tools (like git or bitbucket) and container technologies (apptainer or docker). - They are scalable, you can start testing with just a couple of samples and easily scale up to hundreds or thusands. Processes are run in parallele automatically when possible. - It resumes executions. Automatically checks the processes and can resume from a point of failure without having to re-compute already completed parts. - It’s open source.\n\n** Processes and Channels**\nThe Netxflow workflows are made by combining different processes. This processes can be written in different lenguages as long as they can be executed by a Linux platform. Each process is executed independently, and different processes only comunicate via a first-in, first-out asynchronous queue. These queues are called channels, and any process can defined them as input and output. The interaction between these processes and the execution of the workflow is defined by these input output declarations.\nThis is how a basic Nextflow pipeline process block would look like:\nprocess PROCESS_NAME{\n\n    input:\n      data z\n      data y\n      data x\n\n    // directives\n    container\n\n    script:\n      task1\n      task2\n      task3\n\n    output:\n      output x\n      output y\n      output z\n}\n\n\nExecution abstraction running fastQC\nIn Nextflow, the process defines what comand or script is executed, and the executor determines how that script is runned. By default, the processes are excuted on the local computer. The local executor is very useful in workflow development and testing, but when running real computational workflows other platforms are required, like a high-performance computing or cloud platform.\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz \n\n\nNextflow code example\nScript:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nTo run it:\npixi run nextflow run hello.nf\nThe output:\n N E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ✔\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ✔\nHELLO \nWORLD!\n\n\nModify and resume\nNextflow keeps track of all the processes executed and if you modify some prts of your script, only the processes that are changed will be re-executed. In this case, it will use the cached result from the processes that weren’t changed.\n\n\nWorkflow parameters\nThe workflow parameters are simply declared by prepending the prefix params to a variable name, separated by a ‘.’ (e.g. params.greetings). Their value can be specified on the comand line:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\nClean up Nextflow\nPeriodically, it makes sense to clean up your nextflow working directory. You can do that manually, but the non-descriptive nature of the file system makes that difficult.\nNextflow keeps track of your runs that have been executed from the current directory. The folowing comand will show you the executions log and runtime information of all the runs.\npixi run nextflow log\nYou can clean the porject cache and th work directories with the folowing comand:\nnextflow clean\nYou can also specify the run or runs that you want to eliminate. Here you can find a more extensive explanation of the different options."
  },
  {
    "objectID": "posts/Working-with-environments/index.html",
    "href": "posts/Working-with-environments/index.html",
    "title": "Working with environments",
    "section": "",
    "text": "The different operating systems support different tools. In order to solve this issue, we can use environments, which wil help us make our bioinformatics work more reproducible. This environments specifiy the tools needed to perform the task and the managers inside the environment will make sure to install these tools with their dependencies so that you can run the analysis in your device.\nThere are different types of environments, like Conda, but in this course we are going to focus on Pixi."
  },
  {
    "objectID": "posts/Working-with-environments/index.html#install-pixi",
    "href": "posts/Working-with-environments/index.html#install-pixi",
    "title": "Working with environments",
    "section": "Install Pixi",
    "text": "Install Pixi\nIn order to install pixi just run the following command in your terminal:\ncurl -fsSL https://pixi.sh/install.sh | sh\nYou can find more infromation about pixi here"
  },
  {
    "objectID": "posts/Working-with-environments/index.html#create-an-environment",
    "href": "posts/Working-with-environments/index.html#create-an-environment",
    "title": "Working with environments",
    "section": "Create an environment",
    "text": "Create an environment\nTo create an environment for your project we will ask pixi to initialize a folder named pixi_training. We will also add conda-forge and bioconda channles with the -c flag.\npixi init pixi_training -c conda-forge -c bioconda\nIf you look in the new folder created you find a file called pixi.toml.\n\n1. pixi.toml\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"amrei_pixi_training\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nThis is how the pixi.tomlfile would like like after creating it. This file gives you information about your project:\n\nWorkspace: An overview of the environment: the channels that we added when we called pixi init -c, the name of the environment, the operating system and the version.\nTasks (not covered)\nDependencies: it will list the different tools that you install within the environment.\n\n\n\n2. pixi.lock\nAdding a tool or a dependency to the environemnt will generate another file called pixi.lock.\nThis file contains information about the channels added to you environment, where the packages were dowloaded from, liceses information and more.\n\n\n\n\n\n\nDO NOT DELETE pixi.tomlor pixi.lock files, it will break your environment!"
  },
  {
    "objectID": "posts/Working-with-environments/index.html#adding-dependencies",
    "href": "posts/Working-with-environments/index.html#adding-dependencies",
    "title": "Working with environments",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nThe comand pixi add will tell Pixi to install a specified program for you and it will be added to the dependencies list found in the pixi.toml file that we talked about in the previous section.\npwd\ncd pixi_training\npixi add quarto\nIf we now check the dependencies in the pixi.toml file we would see the new program added quarto = \"&gt;=1.7.32,&lt;2\"(the versions might be different)."
  },
  {
    "objectID": "posts/Working-with-environments/index.html#running-a-package",
    "href": "posts/Working-with-environments/index.html#running-a-package",
    "title": "Working with environments",
    "section": "Running a package",
    "text": "Running a package\nIn order to run a package we use the run command followed by the package.\npixi run quarto --help\nYou can substitute quarto with any other package followed by the pertinent commands."
  }
]